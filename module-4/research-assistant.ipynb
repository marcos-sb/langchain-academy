{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a5c2153",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain-academy/blob/main/module-4/research-assistant.ipynb) [![Open in LangChain Academy](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66e9eba12c7b7688aa3dbb5e_LCA-badge-green.svg)](https://academy.langchain.com/courses/take/intro-to-langgraph/lessons/58239974-lesson-4-research-assistant)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e0a5763f-5f45-4b8f-b3e2-480f46c5721b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Research Assistant\n",
    "\n",
    "## Review\n",
    "\n",
    "We've covered a few major LangGraph themes:\n",
    "\n",
    "* Memory\n",
    "* Human-in-the-loop\n",
    "* Controllability\n",
    "\n",
    "Now, we'll bring these ideas together to tackle one of AI's most popular applications: research automation. \n",
    "\n",
    "Research is often laborious work offloaded to analysts. AI has considerable potential to assist with this.\n",
    "\n",
    "However, research demands customization: raw LLM outputs are often poorly suited for real-world decision-making workflows. \n",
    "\n",
    "Customized, AI-based [research and report generation](https://jxnl.co/writing/2024/06/05/predictions-for-the-future-of-rag/#reports-over-rag) workflows are a promising way to address this.\n",
    "\n",
    "## Goal\n",
    "\n",
    "Our goal is to build a lightweight, multi-agent system around chat models that customizes the research process.\n",
    "\n",
    "`Source Selection` \n",
    "* Users can choose any set of input sources for their research.\n",
    "  \n",
    "`Planning` \n",
    "* Users provide a topic, and the system generates a team of AI analysts, each focusing on one sub-topic.\n",
    "* `Human-in-the-loop` will be used to refine these sub-topics before research begins.\n",
    "  \n",
    "`LLM Utilization`\n",
    "* Each analyst will conduct in-depth interviews with an expert AI using the selected sources.\n",
    "* The interview will be a multi-turn conversation to extract detailed insights as shown in the [STORM](https://arxiv.org/abs/2402.14207) paper.\n",
    "* These interviews will be captured in a using `sub-graphs` with their internal state. \n",
    "   \n",
    "`Research Process`\n",
    "* Experts will gather information to answer analyst questions in `parallel`.\n",
    "* And all interviews will be conducted simultaneously through `map-reduce`.\n",
    "\n",
    "`Output Format` \n",
    "* The gathered insights from each interview will be synthesized into a final report.\n",
    "* We'll use customizable prompts for the report, allowing for a flexible output format. \n",
    "\n",
    "![Screenshot 2024-08-26 at 7.26.33 PM.png](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66dbb164d61c93d48e604091_research-assistant1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f23991e9-51b3-4e9f-86a0-dec16aa7d1e6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install --quiet -U langgraph langchain_openai langchain_community langchain_core tavily-python wikipedia"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a1c01d-87e1-4723-b83e-ebcf937fe914",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ba917800-10e4-4e2a-8e9e-30893b731e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, getpass\n",
    "\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "afe9ff57-0826-4669-b88b-4d0501a509f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI(\n",
    "    model=\"qwen/qwen3-4b-2507\",\n",
    "    base_url=\"http://localhost:1234/v1\",\n",
    "    api_key=\"nothing\",\n",
    "    temperature=0,\n",
    "    max_tokens=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3419257b-2c6b-4d68-ae38-4a266cc02982",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "We'll use [LangSmith](https://docs.langchain.com/langsmith/home) for [tracing](https://docs.langchain.com/langsmith/observability-concepts)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5102cf2e-0ca9-465b-9499-67abb8132e5d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "_set_env(\"LANGSMITH_API_KEY\")\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "os.environ[\"LANGSMITH_PROJECT\"] = \"langchain-academy\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8fe5d93-e353-44bb-be3e-434654bcb7ea",
   "metadata": {},
   "source": [
    "## Generate Analysts: Human-In-The-Loop\n",
    "\n",
    "Create analysts and review them using human-in-the-loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1eee8e60-e548-49b1-88ec-a4f3aef2174e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from typing_extensions import TypedDict\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class Analyst(BaseModel):\n",
    "    affiliation: str = Field(\n",
    "        description=\"Primary affiliation of the analyst.\",\n",
    "    )\n",
    "    name: str = Field(\n",
    "        description=\"Name of the analyst.\"\n",
    "    )\n",
    "    role: str = Field(\n",
    "        description=\"Role of the analyst in the context of the topic.\",\n",
    "    )\n",
    "    description: str = Field(\n",
    "        description=\"Description of the analyst focus, concerns, and motives.\",\n",
    "    )\n",
    "    @property\n",
    "    def persona(self) -> str:\n",
    "        return f\"Name: {self.name}\\nRole: {self.role}\\nAffiliation: {self.affiliation}\\nDescription: {self.description}\\n\"\n",
    "\n",
    "class Perspectives(BaseModel):\n",
    "    analysts: List[Analyst] = Field(\n",
    "        description=\"Comprehensive list of analysts with their roles and affiliations.\",\n",
    "    )\n",
    "\n",
    "class GenerateAnalystsState(TypedDict):\n",
    "    topic: str # Research topic\n",
    "    max_analysts: int # Number of analysts\n",
    "    human_analyst_feedback: str # Human feedback\n",
    "    analysts: List[Analyst] # Analyst asking questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fd088ff5-4c75-412c-85f0-04afd0900bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "from langgraph.graph import START, END, StateGraph\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
    "\n",
    "analyst_instructions=\"\"\"You are tasked with creating a set of AI analyst personas. Follow these instructions carefully:\n",
    "\n",
    "1. First, review the research topic:\n",
    "{topic}\n",
    "        \n",
    "2. Examine any editorial feedback that has been optionally provided to guide creation of the analysts: \n",
    "        \n",
    "{human_analyst_feedback}\n",
    "    \n",
    "3. Determine the most interesting themes based upon documents and / or feedback above.\n",
    "                    \n",
    "4. Pick the top {max_analysts} themes.\n",
    "\n",
    "5. Assign one analyst to each theme.\"\"\"\n",
    "\n",
    "def create_analysts(state: GenerateAnalystsState):\n",
    "    \n",
    "    \"\"\" Create analysts \"\"\"\n",
    "    \n",
    "    topic=state['topic']\n",
    "    max_analysts=state['max_analysts']\n",
    "    human_analyst_feedback=state.get('human_analyst_feedback', '')\n",
    "        \n",
    "    # Enforce structured output\n",
    "    structured_llm = llm.with_structured_output(Perspectives)\n",
    "\n",
    "    # System message\n",
    "    system_message = analyst_instructions.format(topic=topic,\n",
    "                                                            human_analyst_feedback=human_analyst_feedback, \n",
    "                                                            max_analysts=max_analysts)\n",
    "\n",
    "    # Generate question \n",
    "    analysts = structured_llm.invoke([SystemMessage(content=system_message)]+[HumanMessage(content=\"Generate the set of analysts.\")])\n",
    "    \n",
    "    # Write the list of analysis to state\n",
    "    return {\"analysts\": analysts.analysts}\n",
    "\n",
    "def human_feedback(state: GenerateAnalystsState):\n",
    "    \"\"\" No-op node that should be interrupted on \"\"\"\n",
    "    pass\n",
    "\n",
    "def should_continue(state: GenerateAnalystsState):\n",
    "    \"\"\" Return the next node to execute \"\"\"\n",
    "\n",
    "    # Check if human feedback\n",
    "    human_analyst_feedback=state.get('human_analyst_feedback', None)\n",
    "    if human_analyst_feedback:\n",
    "        return \"create_analysts\"\n",
    "    \n",
    "    # Otherwise end\n",
    "    return END\n",
    "\n",
    "# Add nodes and edges \n",
    "builder = StateGraph(GenerateAnalystsState)\n",
    "builder.add_node(\"create_analysts\", create_analysts)\n",
    "builder.add_node(\"human_feedback\", human_feedback)\n",
    "builder.add_edge(START, \"create_analysts\")\n",
    "builder.add_edge(\"create_analysts\", \"human_feedback\")\n",
    "builder.add_conditional_edges(\"human_feedback\", should_continue, [\"create_analysts\", END])\n",
    "\n",
    "# Compile\n",
    "memory = MemorySaver()\n",
    "graph = builder.compile(interrupt_before=['human_feedback'], checkpointer=memory)\n",
    "\n",
    "# View\n",
    "# display(Image(graph.get_graph(xray=1).draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6c22cb05-c436-4358-8f7a-72d722f9b5cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: Dr. Elena Torres\n",
      "Affiliation: Quantitative Research Division, Goldman Sachs\n",
      "Role: Chief Quantitative Strategist\n",
      "Description: Dr. Elena Torres leads the quantitative modeling team at Goldman Sachs, specializing in predictive analytics for financial assets. With a Ph.D. in applied mathematics and over 15 years of experience in algorithmic trading, she focuses on time-series forecasting, machine learning models, and macroeconomic sentiment integration. Her approach to Tesla stock prediction emphasizes statistical rigor, backtesting, and the use of high-frequency data (e.g., trading volume, sentiment, and supply chain metrics) to forecast one-year price movements. She is known for her skepticism toward overfitting and her preference for robust, transparent models that can withstand market regime shifts.\n",
      "--------------------------------------------------\n",
      "Name: Rajiv Mehta\n",
      "Affiliation: Sustainable Finance Group, Morgan Stanley\n",
      "Role: Head of ESG & Technology Impact Analysis\n",
      "Description: Rajiv Mehta brings a deep understanding of the intersection between environmental, social, and governance (ESG) factors and market performance. As a leading expert in clean energy transitions, he evaluates how Tesla’s product innovation, supply chain sustainability, and regulatory exposure (e.g., battery material sourcing, carbon footprint) influence long-term investor sentiment and stock valuation. His analysis of Tesla’s one-year outlook centers on ESG risk exposure, policy tailwinds (e.g., U.S. Inflation Reduction Act), and the company’s ability to scale sustainable mobility solutions under regulatory and public scrutiny.\n",
      "--------------------------------------------------\n",
      "Name: Dr. Amina Diallo\n",
      "Affiliation: Market Sentiment & Behavioral Economics Unit, BNP Paribas\n",
      "Role: Behavioral Finance Analyst\n",
      "Description: Dr. Amina Diallo specializes in behavioral and sentiment-driven market dynamics. She uses natural language processing (NLP) to analyze earnings calls, social media, and news sentiment to predict shifts in investor confidence and market momentum. For Tesla’s one-year forecast, she evaluates how narratives—such as Elon Musk’s public statements, product launches (e.g., Cybertruck, AI Day), and social media buzz—impact short-term volatility and long-term investor expectations. Her model integrates sentiment scoring, network analysis, and herding behavior to identify potential market overreactions or underreactions that traditional models may miss.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Input\n",
    "max_analysts = 3 \n",
    "topic = \"Prediction of Tesla stock price in one year\"\n",
    "thread = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "# Run the graph until the first interruption\n",
    "for event in graph.stream({\"topic\":topic,\"max_analysts\":max_analysts,}, thread, stream_mode=\"values\"):\n",
    "    # Review\n",
    "    analysts = event.get('analysts', '')\n",
    "    if analysts:\n",
    "        for analyst in analysts:\n",
    "            print(f\"Name: {analyst.name}\")\n",
    "            print(f\"Affiliation: {analyst.affiliation}\")\n",
    "            print(f\"Role: {analyst.role}\")\n",
    "            print(f\"Description: {analyst.description}\")\n",
    "            print(\"-\" * 50)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2f81ad23-5656-43e6-b50a-0d7a4f69a60a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('human_feedback',)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get state and look at next node\n",
    "state = graph.get_state(thread)\n",
    "state.next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "72b2a402-fd10-4f26-9a32-3e3c0d4aaf76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'configurable': {'thread_id': '1',\n",
       "  'checkpoint_ns': '',\n",
       "  'checkpoint_id': '1f0bcd25-94c8-6da0-8003-2d0fdafee3b3'}}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We now update the state as if we are the human_feedback node\n",
    "graph.update_state(thread, {\"human_analyst_feedback\": \n",
    "                            \"Add in someone from crypto-finance to have that perspective\"}, as_node=\"human_feedback\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b8816eb9-9906-441b-b552-be71107db14f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: Dr. Elena Torres\n",
      "Affiliation: Quantitative Research Division, Goldman Sachs\n",
      "Role: Chief Quantitative Strategist\n",
      "Description: Dr. Elena Torres leads the quantitative modeling team at Goldman Sachs, specializing in predictive analytics for financial assets. With a Ph.D. in applied mathematics and over 15 years of experience in algorithmic trading, she focuses on time-series forecasting, machine learning models, and macroeconomic sentiment integration. Her approach to Tesla stock prediction emphasizes statistical rigor, backtesting, and the use of high-frequency data (e.g., trading volume, sentiment, and supply chain metrics) to forecast one-year price movements. She is known for her skepticism toward overfitting and her preference for robust, transparent models that can withstand market regime shifts.\n",
      "--------------------------------------------------\n",
      "Name: Rajiv Mehta\n",
      "Affiliation: Sustainable Finance Group, Morgan Stanley\n",
      "Role: Head of ESG & Technology Impact Analysis\n",
      "Description: Rajiv Mehta brings a deep understanding of the intersection between environmental, social, and governance (ESG) factors and market performance. As a leading expert in clean energy transitions, he evaluates how Tesla’s product innovation, supply chain sustainability, and regulatory exposure (e.g., battery material sourcing, carbon footprint) influence long-term investor sentiment and stock valuation. His analysis of Tesla’s one-year outlook centers on ESG risk exposure, policy tailwinds (e.g., U.S. Inflation Reduction Act), and the company’s ability to scale sustainable mobility solutions under regulatory and public scrutiny.\n",
      "--------------------------------------------------\n",
      "Name: Dr. Amina Diallo\n",
      "Affiliation: Market Sentiment & Behavioral Economics Unit, BNP Paribas\n",
      "Role: Behavioral Finance Analyst\n",
      "Description: Dr. Amina Diallo specializes in behavioral and sentiment-driven market dynamics. She uses natural language processing (NLP) to analyze earnings calls, social media, and news sentiment to predict shifts in investor confidence and market momentum. For Tesla’s one-year forecast, she evaluates how narratives—such as Elon Musk’s public statements, product launches (e.g., Cybertruck, AI Day), and social media buzz—impact short-term volatility and long-term investor expectations. Her model integrates sentiment scoring, network analysis, and herding behavior to identify potential market overreactions or underreactions that traditional models may miss.\n",
      "--------------------------------------------------\n",
      "Name: Dr. Elena Torres\n",
      "Affiliation: Quantitative Finance Division, Goldman Sachs\n",
      "Role: Senior Quantitative Strategist\n",
      "Description: Dr. Elena Torres brings deep expertise in time-series modeling, machine learning applied to financial data, and predictive analytics for equities. She specializes in building and validating predictive models for large-cap tech stocks, with a focus on Tesla’s valuation dynamics, earnings trajectory, and macroeconomic sensitivity. Her approach combines statistical forecasting with behavioral finance insights.\n",
      "--------------------------------------------------\n",
      "Name: Rajiv Mehta\n",
      "Affiliation: Tesla Ecosystem & Innovation Research\n",
      "Role: Head of Automotive & AI Strategy\n",
      "Description: Rajiv Mehta provides a forward-looking, technology-driven perspective on Tesla’s product roadmap, including the impact of AI-driven autonomy, Full Self-Driving (FSD) updates, and energy storage innovations. He evaluates how these developments influence long-term revenue streams and market positioning—key drivers of future stock performance.\n",
      "--------------------------------------------------\n",
      "Name: Sofia Chen\n",
      "Affiliation: Crypto & Digital Asset Research Group\n",
      "Role: Head of Crypto & DeFi Strategy\n",
      "Description: Sofia Chen offers a unique, non-traditional lens on Tesla’s future performance by analyzing the intersection of Tesla’s investments in blockchain, digital assets, and decentralized technologies. She assesses how Tesla’s recent forays into crypto (e.g., Bitcoin holdings, potential Web3 ventures) and the broader crypto market sentiment could influence investor sentiment, risk appetite, and long-term valuation.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Continue the graph execution\n",
    "for event in graph.stream(None, thread, stream_mode=\"values\"):\n",
    "    # Review\n",
    "    analysts = event.get('analysts', '')\n",
    "    if analysts:\n",
    "        for analyst in analysts:\n",
    "            print(f\"Name: {analyst.name}\")\n",
    "            print(f\"Affiliation: {analyst.affiliation}\")\n",
    "            print(f\"Role: {analyst.role}\")\n",
    "            print(f\"Description: {analyst.description}\")\n",
    "            print(\"-\" * 50) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a43ac322-5926-4932-8653-68206fec0d2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'configurable': {'thread_id': '1',\n",
       "  'checkpoint_ns': '',\n",
       "  'checkpoint_id': '1f0bcd27-b3d0-6c14-8005-935d51258afc'}}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If we are satisfied, then we simply supply no feedback\n",
    "further_feedack = None\n",
    "graph.update_state(thread, {\"human_analyst_feedback\": \n",
    "                            further_feedack}, as_node=\"human_feedback\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ab034e65-aeee-4723-8d6d-74541b548425",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continue the graph execution to end\n",
    "for event in graph.stream(None, thread, stream_mode=\"updates\"):\n",
    "    print(\"--Node--\")\n",
    "    node_name = next(iter(event.keys()))\n",
    "    print(node_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2f204e8a-285c-4e46-8223-a695caec7764",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_state = graph.get_state(thread)\n",
    "analysts = final_state.values.get('analysts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "59704086-cb3b-42e9-8395-37be6f0d44e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "()"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_state.next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "95717ba3-aa00-48d6-bbb7-5fe4db5919bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: Dr. Elena Torres\n",
      "Affiliation: Quantitative Finance Division, Goldman Sachs\n",
      "Role: Senior Quantitative Strategist\n",
      "Description: Dr. Elena Torres brings deep expertise in time-series modeling, machine learning applied to financial data, and predictive analytics for equities. She specializes in building and validating predictive models for large-cap tech stocks, with a focus on Tesla’s valuation dynamics, earnings trajectory, and macroeconomic sensitivity. Her approach combines statistical forecasting with behavioral finance insights.\n",
      "--------------------------------------------------\n",
      "Name: Rajiv Mehta\n",
      "Affiliation: Tesla Ecosystem & Innovation Research\n",
      "Role: Head of Automotive & AI Strategy\n",
      "Description: Rajiv Mehta provides a forward-looking, technology-driven perspective on Tesla’s product roadmap, including the impact of AI-driven autonomy, Full Self-Driving (FSD) updates, and energy storage innovations. He evaluates how these developments influence long-term revenue streams and market positioning—key drivers of future stock performance.\n",
      "--------------------------------------------------\n",
      "Name: Sofia Chen\n",
      "Affiliation: Crypto & Digital Asset Research Group\n",
      "Role: Head of Crypto & DeFi Strategy\n",
      "Description: Sofia Chen offers a unique, non-traditional lens on Tesla’s future performance by analyzing the intersection of Tesla’s investments in blockchain, digital assets, and decentralized technologies. She assesses how Tesla’s recent forays into crypto (e.g., Bitcoin holdings, potential Web3 ventures) and the broader crypto market sentiment could influence investor sentiment, risk appetite, and long-term valuation.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for analyst in analysts:\n",
    "    print(f\"Name: {analyst.name}\")\n",
    "    print(f\"Affiliation: {analyst.affiliation}\")\n",
    "    print(f\"Role: {analyst.role}\")\n",
    "    print(f\"Description: {analyst.description}\")\n",
    "    print(\"-\" * 50) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2498e4-20ae-4503-9dd0-a4165132b7a7",
   "metadata": {},
   "source": [
    "## Conduct Interview\n",
    "\n",
    "### Generate Question\n",
    "\n",
    "The analyst will ask questions to the expert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e5d5f559-f42e-442b-87cd-dbf0a91abf9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "from typing import Annotated\n",
    "from langgraph.graph import MessagesState\n",
    "\n",
    "class InterviewState(MessagesState):\n",
    "    max_num_turns: int # Number turns of conversation\n",
    "    context: Annotated[list, operator.add] # Source docs\n",
    "    analyst: Analyst # Analyst asking questions\n",
    "    interview: str # Interview transcript\n",
    "    sections: list # Final key we duplicate in outer state for Send() API\n",
    "\n",
    "class SearchQuery(BaseModel):\n",
    "    search_query: str = Field(None, description=\"Search query for retrieval.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1c2e71eb-07ad-4bea-aabc-dbaf551408c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_instructions = \"\"\"You are an analyst tasked with interviewing an expert to learn about a specific topic. \n",
    "\n",
    "Your goal is boil down to interesting and specific insights related to your topic.\n",
    "\n",
    "1. Interesting: Insights that people will find surprising or non-obvious.\n",
    "        \n",
    "2. Specific: Insights that avoid generalities and include specific examples from the expert.\n",
    "\n",
    "Here is your topic of focus and set of goals: {goals}\n",
    "        \n",
    "Begin by introducing yourself using a name that fits your persona, and then ask your question.\n",
    "\n",
    "Continue to ask questions to drill down and refine your understanding of the topic.\n",
    "        \n",
    "When you are satisfied with your understanding, complete the interview with: \"Thank you so much for your help!\"\n",
    "\n",
    "Remember to stay in character throughout your response, reflecting the persona and goals provided to you.\"\"\"\n",
    "\n",
    "def generate_question(state: InterviewState):\n",
    "    \"\"\" Node to generate a question \"\"\"\n",
    "\n",
    "    # Get state\n",
    "    analyst = state[\"analyst\"]\n",
    "    messages = state[\"messages\"]\n",
    "\n",
    "    # Generate question \n",
    "    system_message = question_instructions.format(goals=analyst.persona)\n",
    "    question = llm.invoke([SystemMessage(content=system_message)]+messages)\n",
    "        \n",
    "    # Write messages to state\n",
    "    return {\"messages\": [question]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2ff33a-6232-4a79-8a82-882a645394f5",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Generate Answer: Parallelization\n",
    "\n",
    "The expert will gather information from multiple sources in parallel to answer questions.\n",
    "\n",
    "For example, we can use:\n",
    "\n",
    "* Specific web sites e.g., via [`WebBaseLoader`](https://docs.langchain.com/oss/python/integrations/document_loaders/web_base)\n",
    "* Indexed documents e.g., via [RAG](https://docs.langchain.com/oss/python/langchain/retrieval)\n",
    "* Web search\n",
    "* Wikipedia search\n",
    "\n",
    "You can try different web search tools, like [Tavily](https://tavily.com/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "606ea95b-e811-4299-8b66-835d4016c338",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "_set_env(\"TAVILY_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c61ae74a-f838-4e97-8bd5-48ccd15b7789",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Web search tool\n",
    "from langchain_tavily import TavilySearch  # updated 1.0\n",
    "\n",
    "tavily_search = TavilySearch(max_results=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2d8f760b-5a1a-4fa9-a014-d3fb02bec51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wikipedia search tool\n",
    "from langchain_community.document_loaders import WikipediaLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06cb1603",
   "metadata": {},
   "source": [
    "Now, we create nodes to search the web and wikipedia.\n",
    "\n",
    "We'll also create a node to answer analyst questions.\n",
    "\n",
    "Finally, we'll create nodes to save the full interview and to write a summary (\"section\") of the interview."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9c863768-2278-415b-aef1-96fd18c1b1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import get_buffer_string\n",
    "\n",
    "# Search query writing\n",
    "search_instructions = SystemMessage(content=f\"\"\"You will be given a conversation between an analyst and an expert. \n",
    "\n",
    "Your goal is to generate a well-structured query for use in retrieval and / or web-search related to the conversation.\n",
    "        \n",
    "First, analyze the full conversation.\n",
    "\n",
    "Pay particular attention to the final question posed by the analyst.\n",
    "\n",
    "Convert this final question into a well-structured web search query\"\"\")\n",
    "\n",
    "def search_web(state: InterviewState):\n",
    "    \n",
    "    \"\"\" Retrieve docs from web search \"\"\"\n",
    "\n",
    "    # Search query\n",
    "    structured_llm = llm.with_structured_output(SearchQuery)\n",
    "    search_query = structured_llm.invoke([search_instructions]+state['messages'])\n",
    "    \n",
    "    # Search\n",
    "    #search_docs = tavily_search.invoke(search_query.search_query) # updated 1.0\n",
    "    data = tavily_search.invoke({\"query\": search_query.search_query})\n",
    "    search_docs = data.get(\"results\", data)\n",
    "    \n",
    "\n",
    "     # Format\n",
    "    formatted_search_docs = \"\\n\\n---\\n\\n\".join(\n",
    "        [\n",
    "            f'<Document href=\"{doc[\"url\"]}\"/>\\n{doc[\"content\"]}\\n</Document>'\n",
    "            for doc in search_docs\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return {\"context\": [formatted_search_docs]} \n",
    "\n",
    "def search_wikipedia(state: InterviewState):\n",
    "    \n",
    "    \"\"\" Retrieve docs from wikipedia \"\"\"\n",
    "\n",
    "    # Search query\n",
    "    structured_llm = llm.with_structured_output(SearchQuery)\n",
    "    search_query = structured_llm.invoke([search_instructions]+state['messages'])\n",
    "    \n",
    "    # Search\n",
    "    search_docs = WikipediaLoader(query=search_query.search_query, \n",
    "                                  load_max_docs=2).load()\n",
    "\n",
    "     # Format\n",
    "    formatted_search_docs = \"\\n\\n---\\n\\n\".join(\n",
    "        [\n",
    "            f'<Document source=\"{doc.metadata[\"source\"]}\" page=\"{doc.metadata.get(\"page\", \"\")}\"/>\\n{doc.page_content}\\n</Document>'\n",
    "            for doc in search_docs\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return {\"context\": [formatted_search_docs]} \n",
    "\n",
    "answer_instructions = \"\"\"You are an expert being interviewed by an analyst.\n",
    "\n",
    "Here is the analyst area of focus: {goals}. \n",
    "        \n",
    "You goal is to answer a question posed by the interviewer.\n",
    "\n",
    "To answer question, use this context:\n",
    "        \n",
    "{context}\n",
    "\n",
    "When answering questions, follow these guidelines:\n",
    "        \n",
    "1. Use only the information provided in the context. \n",
    "        \n",
    "2. Do not introduce external information or make assumptions beyond what is explicitly stated in the context.\n",
    "\n",
    "3. The context contain sources at the topic of each individual document.\n",
    "\n",
    "4. Include these sources your answer next to any relevant statements. For example, for source # 1 use [1]. \n",
    "\n",
    "5. List your sources in order at the bottom of your answer. [1] Source 1, [2] Source 2, etc\n",
    "        \n",
    "6. If the source is: <Document source=\"assistant/docs/llama3_1.pdf\" page=\"7\"/>' then just list: \n",
    "        \n",
    "[1] assistant/docs/llama3_1.pdf, page 7 \n",
    "        \n",
    "And skip the addition of the brackets as well as the Document source preamble in your citation.\"\"\"\n",
    "\n",
    "def generate_answer(state: InterviewState):\n",
    "    \n",
    "    \"\"\" Node to answer a question \"\"\"\n",
    "\n",
    "    # Get state\n",
    "    analyst = state[\"analyst\"]\n",
    "    messages = state[\"messages\"]\n",
    "    context = state[\"context\"]\n",
    "\n",
    "    # Answer question\n",
    "    system_message = answer_instructions.format(goals=analyst.persona, context=context)\n",
    "    answer = llm.invoke([SystemMessage(content=system_message)]+messages)\n",
    "            \n",
    "    # Name the message as coming from the expert\n",
    "    answer.name = \"expert\"\n",
    "    \n",
    "    # Append it to state\n",
    "    return {\"messages\": [answer]}\n",
    "\n",
    "def save_interview(state: InterviewState):\n",
    "    \n",
    "    \"\"\" Save interviews \"\"\"\n",
    "\n",
    "    # Get messages\n",
    "    messages = state[\"messages\"]\n",
    "    \n",
    "    # Convert interview to a string\n",
    "    interview = get_buffer_string(messages)\n",
    "    \n",
    "    # Save to interviews key\n",
    "    return {\"interview\": interview}\n",
    "\n",
    "def route_messages(state: InterviewState, \n",
    "                   name: str = \"expert\"):\n",
    "\n",
    "    \"\"\" Route between question and answer \"\"\"\n",
    "    \n",
    "    # Get messages\n",
    "    messages = state[\"messages\"]\n",
    "    max_num_turns = state.get('max_num_turns',2)\n",
    "\n",
    "    # Check the number of expert answers \n",
    "    num_responses = len(\n",
    "        [m for m in messages if isinstance(m, AIMessage) and m.name == name]\n",
    "    )\n",
    "\n",
    "    # End if expert has answered more than the max turns\n",
    "    if num_responses >= max_num_turns:\n",
    "        return 'save_interview'\n",
    "\n",
    "    # This router is run after each question - answer pair \n",
    "    # Get the last question asked to check if it signals the end of discussion\n",
    "    last_question = messages[-2]\n",
    "    \n",
    "    if \"Thank you so much for your help\" in last_question.content:\n",
    "        return 'save_interview'\n",
    "    return \"ask_question\"\n",
    "\n",
    "section_writer_instructions = \"\"\"You are an expert technical writer. \n",
    "            \n",
    "Your task is to create a short, easily digestible section of a report based on a set of source documents.\n",
    "\n",
    "1. Analyze the content of the source documents: \n",
    "- The name of each source document is at the start of the document, with the <Document tag.\n",
    "        \n",
    "2. Create a report structure using markdown formatting:\n",
    "- Use ## for the section title\n",
    "- Use ### for sub-section headers\n",
    "        \n",
    "3. Write the report following this structure:\n",
    "a. Title (## header)\n",
    "b. Summary (### header)\n",
    "c. Sources (### header)\n",
    "\n",
    "4. Make your title engaging based upon the focus area of the analyst: \n",
    "{focus}\n",
    "\n",
    "5. For the summary section:\n",
    "- Set up summary with general background / context related to the focus area of the analyst\n",
    "- Emphasize what is novel, interesting, or surprising about insights gathered from the interview\n",
    "- Create a numbered list of source documents, as you use them\n",
    "- Do not mention the names of interviewers or experts\n",
    "- Aim for approximately 400 words maximum\n",
    "- Use numbered sources in your report (e.g., [1], [2]) based on information from source documents\n",
    "        \n",
    "6. In the Sources section:\n",
    "- Include all sources used in your report\n",
    "- Provide full links to relevant websites or specific document paths\n",
    "- Separate each source by a newline. Use two spaces at the end of each line to create a newline in Markdown.\n",
    "- It will look like:\n",
    "\n",
    "### Sources\n",
    "[1] Link or Document name\n",
    "[2] Link or Document name\n",
    "\n",
    "7. Be sure to combine sources. For example this is not correct:\n",
    "\n",
    "[3] https://ai.meta.com/blog/meta-llama-3-1/\n",
    "[4] https://ai.meta.com/blog/meta-llama-3-1/\n",
    "\n",
    "There should be no redundant sources. It should simply be:\n",
    "\n",
    "[3] https://ai.meta.com/blog/meta-llama-3-1/\n",
    "        \n",
    "8. Final review:\n",
    "- Ensure the report follows the required structure\n",
    "- Include no preamble before the title of the report\n",
    "- Check that all guidelines have been followed\"\"\"\n",
    "\n",
    "def write_section(state: InterviewState):\n",
    "\n",
    "    \"\"\" Node to answer a question \"\"\"\n",
    "\n",
    "    # Get state\n",
    "    interview = state[\"interview\"]\n",
    "    context = state[\"context\"]\n",
    "    analyst = state[\"analyst\"]\n",
    "   \n",
    "    # Write section using either the gathered source docs from interview (context) or the interview itself (interview)\n",
    "    system_message = section_writer_instructions.format(focus=analyst.description)\n",
    "    section = llm.invoke([SystemMessage(content=system_message)]+[HumanMessage(content=f\"Use this source to write your section: {context}\")]) \n",
    "                \n",
    "    # Append it to state\n",
    "    return {\"sections\": [section.content]}\n",
    "\n",
    "# Add nodes and edges \n",
    "interview_builder = StateGraph(InterviewState)\n",
    "interview_builder.add_node(\"ask_question\", generate_question)\n",
    "interview_builder.add_node(\"search_web\", search_web)\n",
    "interview_builder.add_node(\"search_wikipedia\", search_wikipedia)\n",
    "interview_builder.add_node(\"answer_question\", generate_answer)\n",
    "interview_builder.add_node(\"save_interview\", save_interview)\n",
    "interview_builder.add_node(\"write_section\", write_section)\n",
    "\n",
    "# Flow\n",
    "interview_builder.add_edge(START, \"ask_question\")\n",
    "interview_builder.add_edge(\"ask_question\", \"search_web\")\n",
    "interview_builder.add_edge(\"ask_question\", \"search_wikipedia\")\n",
    "interview_builder.add_edge(\"search_web\", \"answer_question\")\n",
    "interview_builder.add_edge(\"search_wikipedia\", \"answer_question\")\n",
    "interview_builder.add_conditional_edges(\"answer_question\", route_messages,['ask_question','save_interview'])\n",
    "interview_builder.add_edge(\"save_interview\", \"write_section\")\n",
    "interview_builder.add_edge(\"write_section\", END)\n",
    "\n",
    "# Interview \n",
    "memory = MemorySaver()\n",
    "interview_graph = interview_builder.compile(checkpointer=memory).with_config(run_name=\"Conduct Interviews\")\n",
    "\n",
    "# View\n",
    "# display(Image(interview_graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "50f382f1-6e93-48d0-a44a-1094d26ccb1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Analyst(affiliation='Quantitative Finance Division, Goldman Sachs', name='Dr. Elena Torres', role='Senior Quantitative Strategist', description='Dr. Elena Torres brings deep expertise in time-series modeling, machine learning applied to financial data, and predictive analytics for equities. She specializes in building and validating predictive models for large-cap tech stocks, with a focus on Tesla’s valuation dynamics, earnings trajectory, and macroeconomic sensitivity. Her approach combines statistical forecasting with behavioral finance insights.')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pick one analyst\n",
    "analysts[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3750ac4f-f458-4b2d-8bad-32ce34895758",
   "metadata": {},
   "source": [
    "Here, we run the interview passing an index of the llama3.1 paper, which is related to our topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b2242d4e-8430-4de9-8cf7-3ad2f9a22b28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Tesla’s Sentiment-Driven Volatility: How Social Media and News Shape Stock Movements\n",
       "\n",
       "### Summary  \n",
       "Social media sentiment—particularly from Elon Musk’s tweets and broader public discourse—plays a significant role in shaping Tesla (TSLA) stock price dynamics. Research has established a measurable correlation between tweet sentiment scores and short-term stock movements, with positive sentiment showing a strong, statistically significant link to upward price adjustments [1]. This connection extends beyond individual tweets; studies have applied sentiment analysis to aggregated news sources, revealing that public perception of Tesla’s performance and future outlook can precede official earnings reports in influencing market behavior [4].  \n",
       "\n",
       "What makes this dynamic particularly compelling is the non-linear, often abrupt nature of these reactions. For instance, while traditional financial models may lag in capturing sentiment shifts, real-time analysis shows that positive sentiment spikes—such as those following Musk’s public endorsements—can trigger immediate buying pressure, pushing TSLA prices above technical resistance levels. This creates a feedback loop where market sentiment reinforces itself, amplifying volatility.  \n",
       "\n",
       "Despite this, the overall market sentiment toward Tesla remains cautiously optimistic. CoinCodex’s 2025–2030 price forecast suggests a potential target of $373.74 in 2025, with a bullish outlook, though it currently advises against buying TSLA as a strong entry point [2]. Meanwhile, 27 analysts have issued a consensus “Hold” rating, indicating skepticism about near-term growth momentum despite positive sentiment trends [5]. This divergence highlights a key insight: while social media sentiment can drive short-term price action, long-term valuation depends on tangible metrics like earnings, production efficiency, and competitive positioning.  \n",
       "\n",
       "The integration of sentiment analysis into predictive models offers a powerful tool for understanding Tesla’s unique market behavior—where emotional and behavioral factors outweigh traditional financial indicators. This approach underscores the importance of monitoring both digital discourse and macroeconomic trends in forecasting future performance.\n",
       "\n",
       "### Sources  \n",
       "[1] https://www.researchgate.net/publication/392749149_The_Impact_of_Social_Media_Sentiment_on_Stock_Price_Changes  \n",
       "[2] https://coincodex.com/stock/TSLA/price-prediction/  \n",
       "[3] https://www.researchgate.net/publication/365379302_Tesla_Inc_Stock_Prediction_using_Sentiment_Analysis  \n",
       "[4] https://www.atlantis-"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Markdown\n",
    "messages = [HumanMessage(f\"So you said you were writing an article on {topic}?\")]\n",
    "thread = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "interview = interview_graph.invoke({\"analyst\": analysts[0], \"messages\": messages, \"max_num_turns\": 2}, thread)\n",
    "Markdown(interview['sections'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b739e87-68bb-4e96-a86a-704e84240a6c",
   "metadata": {},
   "source": [
    "### Parallelize interviews: Map-Reduce\n",
    "\n",
    "We parallelize the interviews via the `Send()` API, a map step.\n",
    "\n",
    "We combine them into the report body in a reduce step.\n",
    "\n",
    "### Finalize\n",
    "\n",
    "We add a final step to write an intro and conclusion to the final report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6a0042f9-5b9f-441a-9e8d-7d8189f44140",
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "from typing import List, Annotated\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "class ResearchGraphState(TypedDict):\n",
    "    topic: str # Research topic\n",
    "    max_analysts: int # Number of analysts\n",
    "    human_analyst_feedback: str # Human feedback\n",
    "    analysts: List[Analyst] # Analyst asking questions\n",
    "    sections: Annotated[list, operator.add] # Send() API key\n",
    "    introduction: str # Introduction for the final report\n",
    "    content: str # Content for the final report\n",
    "    conclusion: str # Conclusion for the final report\n",
    "    final_report: str # Final report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c2224592-d2ff-469d-97bd-928809f896d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.types import Send # updated in 1.0\n",
    "def initiate_all_interviews(state: ResearchGraphState):\n",
    "    \"\"\" This is the \"map\" step where we run each interview sub-graph using Send API \"\"\"    \n",
    "\n",
    "    # Check if human feedback\n",
    "    human_analyst_feedback=state.get('human_analyst_feedback')\n",
    "    if human_analyst_feedback:\n",
    "        # Return to create_analysts\n",
    "        return \"create_analysts\"\n",
    "\n",
    "    # Otherwise kick off interviews in parallel via Send() API\n",
    "    else:\n",
    "        topic = state[\"topic\"]\n",
    "        return [Send(\"conduct_interview\", {\"analyst\": analyst,\n",
    "                                           \"messages\": [HumanMessage(\n",
    "                                               content=f\"So you said you were writing an article on {topic}?\"\n",
    "                                           )\n",
    "                                                       ]}) for analyst in state[\"analysts\"]]\n",
    "\n",
    "report_writer_instructions = \"\"\"You are a technical writer creating a report on this overall topic: \n",
    "\n",
    "{topic}\n",
    "    \n",
    "You have a team of analysts. Each analyst has done two things: \n",
    "\n",
    "1. They conducted an interview with an expert on a specific sub-topic.\n",
    "2. They write up their finding into a memo.\n",
    "\n",
    "Your task: \n",
    "\n",
    "1. You will be given a collection of memos from your analysts.\n",
    "2. Think carefully about the insights from each memo.\n",
    "3. Consolidate these into a crisp overall summary that ties together the central ideas from all of the memos. \n",
    "4. Summarize the central points in each memo into a cohesive single narrative.\n",
    "\n",
    "To format your report:\n",
    " \n",
    "1. Use markdown formatting. \n",
    "2. Include no pre-amble for the report.\n",
    "3. Use no sub-heading. \n",
    "4. Start your report with a single title header: ## Insights\n",
    "5. Do not mention any analyst names in your report.\n",
    "6. Preserve any citations in the memos, which will be annotated in brackets, for example [1] or [2].\n",
    "7. Create a final, consolidated list of sources and add to a Sources section with the `## Sources` header.\n",
    "8. List your sources in order and do not repeat.\n",
    "\n",
    "[1] Source 1\n",
    "[2] Source 2\n",
    "\n",
    "Here are the memos from your analysts to build your report from: \n",
    "\n",
    "{context}\"\"\"\n",
    "\n",
    "def write_report(state: ResearchGraphState):\n",
    "    # Full set of sections\n",
    "    sections = state[\"sections\"]\n",
    "    topic = state[\"topic\"]\n",
    "\n",
    "    # Concat all sections together\n",
    "    formatted_str_sections = \"\\n\\n\".join([f\"{section}\" for section in sections])\n",
    "    \n",
    "    # Summarize the sections into a final report\n",
    "    system_message = report_writer_instructions.format(topic=topic, context=formatted_str_sections)    \n",
    "    report = llm.invoke([SystemMessage(content=system_message)]+[HumanMessage(content=f\"Write a report based upon these memos.\")]) \n",
    "    return {\"content\": report.content}\n",
    "\n",
    "intro_conclusion_instructions = \"\"\"You are a technical writer finishing a report on {topic}\n",
    "\n",
    "You will be given all of the sections of the report.\n",
    "\n",
    "You job is to write a crisp and compelling introduction or conclusion section.\n",
    "\n",
    "The user will instruct you whether to write the introduction or conclusion.\n",
    "\n",
    "Include no pre-amble for either section.\n",
    "\n",
    "Target around 100 words, crisply previewing (for introduction) or recapping (for conclusion) all of the sections of the report.\n",
    "\n",
    "Use markdown formatting. \n",
    "\n",
    "For your introduction, create a compelling title and use the # header for the title.\n",
    "\n",
    "For your introduction, use ## Introduction as the section header. \n",
    "\n",
    "For your conclusion, use ## Conclusion as the section header.\n",
    "\n",
    "Here are the sections to reflect on for writing: {formatted_str_sections}\"\"\"\n",
    "\n",
    "def write_introduction(state: ResearchGraphState):\n",
    "    # Full set of sections\n",
    "    sections = state[\"sections\"]\n",
    "    topic = state[\"topic\"]\n",
    "\n",
    "    # Concat all sections together\n",
    "    formatted_str_sections = \"\\n\\n\".join([f\"{section}\" for section in sections])\n",
    "    \n",
    "    # Summarize the sections into a final report\n",
    "    \n",
    "    instructions = intro_conclusion_instructions.format(topic=topic, formatted_str_sections=formatted_str_sections)    \n",
    "    intro = llm.invoke([instructions]+[HumanMessage(content=f\"Write the report introduction\")]) \n",
    "    return {\"introduction\": intro.content}\n",
    "\n",
    "def write_conclusion(state: ResearchGraphState):\n",
    "    # Full set of sections\n",
    "    sections = state[\"sections\"]\n",
    "    topic = state[\"topic\"]\n",
    "\n",
    "    # Concat all sections together\n",
    "    formatted_str_sections = \"\\n\\n\".join([f\"{section}\" for section in sections])\n",
    "    \n",
    "    # Summarize the sections into a final report\n",
    "    \n",
    "    instructions = intro_conclusion_instructions.format(topic=topic, formatted_str_sections=formatted_str_sections)    \n",
    "    conclusion = llm.invoke([instructions]+[HumanMessage(content=f\"Write the report conclusion\")]) \n",
    "    return {\"conclusion\": conclusion.content}\n",
    "\n",
    "def finalize_report(state: ResearchGraphState):\n",
    "    \"\"\" The is the \"reduce\" step where we gather all the sections, combine them, and reflect on them to write the intro/conclusion \"\"\"\n",
    "    # Save full final report\n",
    "    content = state[\"content\"]\n",
    "    if content.startswith(\"## Insights\"):\n",
    "        content = content.strip(\"## Insights\")\n",
    "    if \"## Sources\" in content:\n",
    "        try:\n",
    "            content, sources = content.split(\"\\n## Sources\\n\")\n",
    "        except:\n",
    "            sources = None\n",
    "    else:\n",
    "        sources = None\n",
    "\n",
    "    final_report = state[\"introduction\"] + \"\\n\\n---\\n\\n\" + content + \"\\n\\n---\\n\\n\" + state[\"conclusion\"]\n",
    "    if sources is not None:\n",
    "        final_report += \"\\n\\n## Sources\\n\" + sources\n",
    "    return {\"final_report\": final_report}\n",
    "\n",
    "# Add nodes and edges \n",
    "builder = StateGraph(ResearchGraphState)\n",
    "builder.add_node(\"create_analysts\", create_analysts)\n",
    "builder.add_node(\"human_feedback\", human_feedback)\n",
    "builder.add_node(\"conduct_interview\", interview_builder.compile())\n",
    "builder.add_node(\"write_report\",write_report)\n",
    "builder.add_node(\"write_introduction\",write_introduction)\n",
    "builder.add_node(\"write_conclusion\",write_conclusion)\n",
    "builder.add_node(\"finalize_report\",finalize_report)\n",
    "\n",
    "# Logic\n",
    "builder.add_edge(START, \"create_analysts\")\n",
    "builder.add_edge(\"create_analysts\", \"human_feedback\")\n",
    "builder.add_conditional_edges(\"human_feedback\", initiate_all_interviews, [\"create_analysts\", \"conduct_interview\"])\n",
    "builder.add_edge(\"conduct_interview\", \"write_report\")\n",
    "builder.add_edge(\"conduct_interview\", \"write_introduction\")\n",
    "builder.add_edge(\"conduct_interview\", \"write_conclusion\")\n",
    "builder.add_edge([\"write_conclusion\", \"write_report\", \"write_introduction\"], \"finalize_report\")\n",
    "builder.add_edge(\"finalize_report\", END)\n",
    "\n",
    "# Compile\n",
    "memory = MemorySaver()\n",
    "graph = builder.compile(interrupt_before=['human_feedback'], checkpointer=memory)\n",
    "# display(Image(graph.get_graph(xray=1).draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b64ba9a-2b5e-40e1-a778-0f635aa3f6d0",
   "metadata": {},
   "source": [
    "Let's ask an open-ended question about LangGraph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "362932ee-4106-4a2d-a32d-b812eafcf9df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: Dr. Elena Marquez\n",
      "Affiliation: LangGraph Core Team\n",
      "Role: Chief Architect\n",
      "Description: Dr. Elena Marquez leads the architectural design and innovation of LangGraph, with deep expertise in scalable agent frameworks and long-context reasoning. She brings a systems-level perspective to how agents can be orchestrated across complex workflows, emphasizing modularity, memory, and dynamic state management. Her work focuses on enabling seamless, human-like reasoning in AI agents through structured graph-based execution.\n",
      "--------------------------------------------------\n",
      "Name: Raj Patel\n",
      "Affiliation: Enterprise AI Solutions\n",
      "Role: Product Strategy Lead\n",
      "Description: Raj Patel specializes in translating LangGraph’s technical capabilities into real-world business value. He evaluates how LangGraph enables end-to-end automation, customer service bots, and internal knowledge systems—particularly in large enterprises where agent reliability, auditability, and compliance matter. His focus is on adoption pathways, ROI, and integration with existing enterprise tools like CRM and LMS.\n",
      "--------------------------------------------------\n",
      "Name: Dr. Lina Chen\n",
      "Affiliation: Research & Innovation Lab\n",
      "Role: AI Researcher\n",
      "Description: Dr. Lina Chen explores the frontier of agent autonomy and emergent behavior in LangGraph. She investigates how graph-based state management enables novel forms of reasoning, such as self-correction, goal drift detection, and multi-agent collaboration. Her work pushes the boundaries of what AI agents can achieve—beyond task execution to adaptive, reflective intelligence.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Inputs\n",
    "max_analysts = 3 \n",
    "topic = \"The benefits of adopting LangGraph as an agent framework\"\n",
    "thread = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "# Run the graph until the first interruption\n",
    "for event in graph.stream({\"topic\":topic,\n",
    "                           \"max_analysts\":max_analysts}, \n",
    "                          thread, \n",
    "                          stream_mode=\"values\"):\n",
    "    \n",
    "    analysts = event.get('analysts', '')\n",
    "    if analysts:\n",
    "        for analyst in analysts:\n",
    "            print(f\"Name: {analyst.name}\")\n",
    "            print(f\"Affiliation: {analyst.affiliation}\")\n",
    "            print(f\"Role: {analyst.role}\")\n",
    "            print(f\"Description: {analyst.description}\")\n",
    "            print(\"-\" * 50)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ac521a5f-5a4f-44f9-8af9-d05228e20882",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'configurable': {'thread_id': '1',\n",
       "  'checkpoint_ns': '',\n",
       "  'checkpoint_id': '1f0ad478-2e3b-6372-8002-87845d0b55a4'}}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We now update the state as if we are the human_feedback node\n",
    "graph.update_state(thread, {\"human_analyst_feedback\": \n",
    "                                \"Add in the CEO of gen ai native startup\"}, as_node=\"human_feedback\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a3be311f-62ee-49e7-b037-75c53d8960a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: Dr. Emily Carter\n",
      "Affiliation: Tech Innovators Inc.\n",
      "Role: Technology Adoption Specialist\n",
      "Description: Dr. Carter focuses on the strategic benefits of adopting new technologies like LangGraph. She is particularly interested in how LangGraph can streamline processes, improve efficiency, and provide a competitive edge to organizations. Her analysis often includes case studies and data-driven insights to support the adoption of innovative frameworks.\n",
      "--------------------------------------------------\n",
      "Name: Mr. Raj Patel\n",
      "Affiliation: Data Security Solutions\n",
      "Role: Cybersecurity Analyst\n",
      "Description: Mr. Patel is concerned with the security implications of adopting new frameworks such as LangGraph. His focus is on understanding how LangGraph can enhance or compromise data security within an organization. He evaluates the framework's security features, potential vulnerabilities, and compliance with industry standards.\n",
      "--------------------------------------------------\n",
      "Name: Dr. Lisa Nguyen\n",
      "Affiliation: AI Ethics Consortium\n",
      "Role: Ethical AI Researcher\n",
      "Description: Dr. Nguyen examines the ethical considerations of implementing agent frameworks like LangGraph. Her work involves assessing the impact of LangGraph on privacy, bias, and transparency in AI systems. She advocates for responsible AI practices and ensures that the adoption of such technologies aligns with ethical guidelines and societal values.\n",
      "--------------------------------------------------\n",
      "Name: Dr. Emily Carter\n",
      "Affiliation: LangGraph Research Institute\n",
      "Role: Framework Efficiency Specialist\n",
      "Description: Dr. Carter focuses on the technical efficiencies and performance improvements that LangGraph offers as an agent framework. Her primary concern is how LangGraph can optimize computational resources and improve the speed and accuracy of AI models. She is motivated by the potential to enhance AI systems' capabilities while reducing operational costs.\n",
      "--------------------------------------------------\n",
      "Name: Raj Patel\n",
      "Affiliation: Tech Innovators Magazine\n",
      "Role: Industry Trends Analyst\n",
      "Description: Raj Patel analyzes the broader industry trends and the strategic advantages of adopting LangGraph. His focus is on how LangGraph positions itself within the competitive landscape of AI frameworks and its potential to drive innovation. He is particularly interested in how LangGraph can influence market dynamics and the adoption rates among tech companies.\n",
      "--------------------------------------------------\n",
      "Name: Sophia Zhang\n",
      "Affiliation: AI Pioneers Inc.\n",
      "Role: CEO of Gen AI Native Startup\n",
      "Description: Sophia Zhang is the CEO of a startup that is native to generative AI technologies. Her focus is on the practical applications and business benefits of integrating LangGraph into her company's operations. She is concerned with how LangGraph can enhance product offerings, improve customer experiences, and provide a competitive edge in the rapidly evolving AI market. Her motivation is to leverage cutting-edge technology to drive business growth and innovation.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Check\n",
    "for event in graph.stream(None, thread, stream_mode=\"values\"):\n",
    "    analysts = event.get('analysts', '')\n",
    "    if analysts:\n",
    "        for analyst in analysts:\n",
    "            print(f\"Name: {analyst.name}\")\n",
    "            print(f\"Affiliation: {analyst.affiliation}\")\n",
    "            print(f\"Role: {analyst.role}\")\n",
    "            print(f\"Description: {analyst.description}\")\n",
    "            print(\"-\" * 50)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0af41f54-88d9-4597-98b0-444c08322095",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'configurable': {'thread_id': '1',\n",
       "  'checkpoint_ns': '',\n",
       "  'checkpoint_id': '1f0bcd5d-27af-61d2-8002-7eb6ac4726d0'}}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Confirm we are happy\n",
    "graph.update_state(thread, {\"human_analyst_feedback\": \n",
    "                            None}, as_node=\"human_feedback\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "37123ca7-c20b-43c1-9a71-39ba344e7ca6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Node--\n",
      "conduct_interview\n",
      "--Node--\n",
      "conduct_interview\n",
      "--Node--\n",
      "conduct_interview\n",
      "--Node--\n",
      "write_conclusion\n",
      "--Node--\n",
      "write_introduction\n",
      "--Node--\n",
      "write_report\n",
      "--Node--\n",
      "finalize_report\n"
     ]
    }
   ],
   "source": [
    "# Continue\n",
    "for event in graph.stream(None, thread, stream_mode=\"updates\"):\n",
    "    print(\"--Node--\")\n",
    "    node_name = next(iter(event.keys()))\n",
    "    print(node_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f8f66ad8-80fd-4eb2-96b6-6ae9dffd060c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# LangGraph: Revolutionizing AI Agent Development\n",
       "\n",
       "## Introduction\n",
       "\n",
       "In the dynamic realm of AI development, LangGraph emerges as a transformative agent framework, offering unparalleled efficiency and capability enhancements. By adopting a graphical approach, LangGraph excels in managing complex, multi-step tasks, crucial for sophisticated AI agents. It integrates declarative and imperative APIs, supporting stateful, multi-agent applications with features like memory storage and fault tolerance. This framework not only reduces development time but also improves system quality, seamlessly integrating with large language models to enhance user experience. LangGraph's strategic advantages, including hybrid integration and open-source infrastructure, position it as a pivotal player in driving business growth and innovation across industries.\n",
       "\n",
       "---\n",
       "\n",
       "\n",
       "\n",
       "LangGraph emerges as a transformative agent framework in the AI landscape, offering significant benefits in enhancing AI efficiency, providing strategic advantages, and fostering business growth and innovation. Its graphical approach to constructing agent behaviors allows for the management of complex, multi-step tasks with backtracking and conditional branching, which is essential for developing sophisticated AI agents capable of operating autonomously and efficiently [1]. This graphical representation simplifies the design and implementation of complex workflows, reducing development time and improving system quality [4].\n",
       "\n",
       "LangGraph's orchestration framework integrates both declarative and imperative APIs, supporting features such as short-term and long-term memory storage, human-in-the-loop processes, and fault tolerance. These capabilities are crucial for maintaining context and ensuring the reliability of AI workflows, making LangGraph a robust foundation for building stateful, multi-agent applications [2]. The framework's ability to integrate seamlessly with large language models (LLMs) further enhances its utility, enabling the creation of intelligent AI agents that can understand and generate human language, thus improving user experience and operational efficiency across various industries [5].\n",
       "\n",
       "Strategically, LangGraph stands out for its ability to handle complex conditional logic and multi-step reasoning, making it ideal for applications requiring advanced state management and long-term system evolution. This positions LangGraph as a preferred choice for enterprises with experienced AI/ML engineers, supporting extended development cycles for sophisticated capabilities [2]. Its potential for hybrid integration with other frameworks allows organizations to create comprehensive, enterprise-ready AI solutions, facilitating gradual migration from existing implementations and risk mitigation through incremental adoption [2].\n",
       "\n",
       "LangGraph's open-source nature and its foundation within the LangChain ecosystem enhance its appeal, providing a robust infrastructure for stateful agents and supporting integration with diverse data sources, APIs, and workflows. This flexibility and modularity make LangGraph not just a tool but a platform that others can build upon, enabling the deployment of multi-agent applications across a wide range of businesses [4].\n",
       "\n",
       "In conclusion, LangGraph offers a unique set of tools and structures that empower businesses to harness the full potential of AI technologies. By integrating LangGraph into their operations, companies can enhance their product offerings, improve customer experiences, and maintain a competitive edge in the AI market. Its graphical approach, robust orchestration capabilities, and strategic advantages position LangGraph as a leading framework for optimizing AI systems' performance and driving innovation across industries.\n",
       "\n",
       "\n",
       "---\n",
       "\n",
       "## Conclusion\n",
       "\n",
       "LangGraph emerges as a transformative agent framework in the AI landscape, offering significant advantages for optimizing AI systems. By employing a graphical approach, LangGraph enhances the efficiency and capability of AI models, particularly in managing complex, multi-step tasks. Its robust orchestration framework, integrating declarative and imperative APIs, supports stateful, multi-agent applications with features like memory storage and fault tolerance. This not only reduces development time but also improves system quality and operational efficiency. LangGraph's seamless integration with large language models further empowers businesses to innovate and maintain a competitive edge, making it a strategic asset for AI-driven growth and innovation.\n",
       "\n",
       "## Sources\n",
       "[1] https://community.latenode.com/t/what-are-the-main-advantages-of-choosing-langgraph-for-ai-agent-development/31000  \n",
       "[2] https://blog.langchain.com/how-to-think-about-agent-frameworks/  \n",
       "[3] https://blog.algoanalytics.com/2025/05/21/langgraph-the-framework-for-intelligent-ai-workflows/  \n",
       "[4] https://medium.com/@ken_lin/langgraph-a-framework-for-building-stateful-multi-agent-llm-applications-a51d5eb68d03  \n",
       "[5] https://www.rapidinnovation.io/post/ai-agents-in-langgraph  \n",
       "[6] https://artizen.com/insights/thought-leadership/ai-agent-frameworks  \n",
       "[7] https://www.leanware.co/insights/langchain-vs-langgraph-comparison  \n",
       "[8] https://interviewkickstart.com/blogs/articles/langgraph-for-retail-ai-agent  \n",
       "[9] https://medium.com/@takafumi.endo/langchain-why-its-the-foundation-of-ai-agent-development-in-the-enterprise-era-f082717c56d3  \n",
       "[10] https://www.scalablepath.com/machine-learning/langgrap"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Markdown\n",
    "final_state = graph.get_state(thread)\n",
    "report = final_state.values.get('final_report')\n",
    "Markdown(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9bf8edd-fb42-496c-9bdb-3f5d7b4d79d3",
   "metadata": {},
   "source": [
    "We can look at the trace:\n",
    "\n",
    "https://smith.langchain.com/public/2933a7bb-bcef-4d2d-9b85-cc735b22ca0c/r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808bd094",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
